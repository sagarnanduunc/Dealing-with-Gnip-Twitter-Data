{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Gnip Data\n",
    "\n",
    "### Objective:\n",
    "- Extract Tweets and Retweets from JSON files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import rarfile\n",
    "import gzip\n",
    "import os\n",
    "import tarfile\n",
    "import re\n",
    "import matplotlib as plt\n",
    "from collections import Counter\n",
    "import csv\n",
    "import math\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to extract Tweets and Retweets for Gnip Data?\n",
    "\n",
    "Ideally, if you want to work on pandas dataframe then extract everything in a dictionary and then convert that to a dataframe. This makes the process very fast, instead of adding one row at a time into a dictionary.  \n",
    "  \n",
    "Below are the functions to extract some useful information of tweets and retweets **ONLY FOR DATA PULLED USING GNIP**.  \n",
    "*Field names in GNIP and Public Twitter API are different. So, you can convert if you'd like and use the same functions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Retweet Info\n",
    "def updateRetweetDict(tweetExtract, tweets):\n",
    "    for tweet in tweets:\n",
    "        if tweet['verb']=='share':\n",
    "            tweetExtract['actorId'].append(re.findall('\\d+', tweet['actor']['id'])[0]) #Extract only the id number\n",
    "            tweetExtract['displayName'].append(tweet['actor']['displayName'])\n",
    "            tweetExtract['screenName'].append(tweet['actor']['preferredUsername'])\n",
    "            tweetExtract['statusesCount'].append(tweet['actor']['statusesCount'])\n",
    "            tweetExtract['favoritesCount'].append(tweet['actor']['favoritesCount'])\n",
    "            tweetExtract['friendsCount'].append(tweet['actor']['friendsCount'])\n",
    "            tweetExtract['followersCount'].append(tweet['actor']['followersCount'])\n",
    "            tweetExtract['listedCounts'].append(tweet['actor']['listedCount'])\n",
    "            if 'languages' not in tweet['actor']:\n",
    "                tweetExtract['actorLanguages'].append([])\n",
    "            else:\n",
    "                tweetExtract['actorLanguages'].append(tweet['actor']['languages'])\n",
    "            if 'summary' not in tweet['actor']:\n",
    "                tweetExtract['summary'].append(None)\n",
    "            else:\n",
    "                tweetExtract['summary'].append(tweet['actor']['summary'])\n",
    "            tweetExtract['createdAt'].append(tweet['actor']['postedTime'])\n",
    "            tweetExtract['verified'].append(tweet['actor']['verified'])\n",
    "            if 'location' not in tweet['actor']:\n",
    "                tweetExtract['location'].append(\"null\")\n",
    "                tweetExtract['locationType'].append(\"null\")\n",
    "            else:\n",
    "                tweetExtract['location'].append(tweet['actor']['location']['displayName'])\n",
    "                tweetExtract['locationType'].append(tweet['actor']['location']['objectType'])\n",
    "            tweetExtract['verb'].append(tweet['verb'])\n",
    "            tweetExtract['tweetId'].append(re.findall(':\\d+',tweet['id'])[0][1:]) #Extract only the id number\n",
    "            tweetExtract['tweetFavCount'].append(tweet['favoritesCount'])\n",
    "            tweetExtract['generator'].append(tweet['generator']['displayName'])\n",
    "            tweetExtract['postedTime'].append(tweet['postedTime'])\n",
    "            tweetExtract['retweetCount'].append(tweet['retweetCount'])\n",
    "            tweetExtract['object.body'].append(tweet['object']['body']) \n",
    "            tweetExtract['object.tweetId'].append(re.findall(':\\d+',tweet['object']['id'])[0][1:]) #Extract only the id number\n",
    "            tweetExtract['object.actorId'].append(re.findall('\\d+', tweet['object']['actor']['id'])[0]) #Extract only the id number\n",
    "            tweetExtract['object.postedTime'].append(tweet['object']['postedTime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Body of Original Tweet for the Retweet is sometimes important because it is possible that Tweet may not belong to the timeline used to filter data.  \n",
    "But we got the retweet for such tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Tweet Info\n",
    "def updateTweetDict(tweetExtract, tweets):\n",
    "    for tweet in tweets:\n",
    "        if tweet['verb']=='post':\n",
    "            tweetExtract['actorId'].append(re.findall('\\d+', tweet['actor']['id'])[0]) #Extract only the id number\n",
    "            tweetExtract['displayName'].append(tweet['actor']['displayName'])\n",
    "            tweetExtract['screenName'].append(tweet['actor']['preferredUsername'])\n",
    "            tweetExtract['statusesCount'].append(tweet['actor']['statusesCount'])\n",
    "            tweetExtract['favoritesCount'].append(tweet['actor']['favoritesCount'])\n",
    "            tweetExtract['friendsCount'].append(tweet['actor']['friendsCount'])\n",
    "            tweetExtract['followersCount'].append(tweet['actor']['followersCount'])\n",
    "            tweetExtract['listedCounts'].append(tweet['actor']['listedCount'])\n",
    "            if 'languages' not in tweet['actor']:\n",
    "                tweetExtract['actorLanguages'].append([])\n",
    "            else:\n",
    "                tweetExtract['actorLanguages'].append(tweet['actor']['languages'])\n",
    "            if 'summary' not in tweet['actor']:\n",
    "                tweetExtract['summary'].append(None)\n",
    "            else:\n",
    "                tweetExtract['summary'].append(tweet['actor']['summary'])\n",
    "            tweetExtract['createdAt'].append(tweet['actor']['postedTime'])\n",
    "            tweetExtract['verified'].append(tweet['actor']['verified'])\n",
    "            if 'location' not in tweet['actor']:\n",
    "                tweetExtract['location'].append(\"null\")\n",
    "                tweetExtract['locationType'].append(\"null\")\n",
    "            else:\n",
    "                tweetExtract['location'].append(tweet['actor']['location']['displayName'])\n",
    "                tweetExtract['locationType'].append(tweet['actor']['location']['objectType'])\n",
    "            tweetExtract['body'].append(tweet['body'])\n",
    "            tweetExtract['verb'].append(tweet['verb'])\n",
    "            tweetExtract['tweetId'].append(re.findall(':\\d+',tweet['id'])[0][1:]) #Extract only the id number\n",
    "            tweetExtract['tweetFavCount'].append(tweet['favoritesCount'])\n",
    "            tweetExtract['generator'].append(tweet['generator']['displayName'])\n",
    "            tweetExtract['hashtags'].append([hashtag['text'] for hashtag in tweet['twitter_entities']['hashtags']])\n",
    "            tweetExtract['mentionIds'].append([mention['id_str'] for mention in tweet['twitter_entities']['user_mentions']])\n",
    "            tweetExtract['mentionScreenNames'].append([mention['screen_name'] for mention in tweet['twitter_entities']['user_mentions']])\n",
    "            if 'inReplyTo' not in tweet: # If Post is a reply\n",
    "                tweetExtract['inReplyTo'].append(\"null\")\n",
    "            else:\n",
    "                tweetExtract['inReplyTo'].append(re.findall('/\\d+',tweet['inReplyTo']['link'])[0][1:]) #Extract only the id number\n",
    "            if 'twitter_quoted_status' not in tweet: # if post is a Quoted Tweet\n",
    "                tweetExtract['quotedTweetId'].append(\"null\")\n",
    "                tweetExtract['quotedTweetUserId'].append(\"null\")\n",
    "            else:\n",
    "                tweetExtract['quotedTweetId'].append(re.findall(':\\d+',tweet['twitter_quoted_status']['id'])[0][1:]) #Extract only the id number\n",
    "                tweetExtract['quotedTweetUserId'].append(re.findall('\\d+', tweet['twitter_quoted_status']['actor']['id'])[0]) #Extract only the id number\n",
    "            tweetExtract['tweetLanguage'].append(tweet['twitter_lang'])\n",
    "            tweetExtract['postedTime'].append(tweet['postedTime'])\n",
    "            tweetExtract['retweetCount'].append(tweet['retweetCount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Activity on Twitter:\n",
    "- Tweet: Write an original post\n",
    "- Retweet: Similar to share on Facebook (shows echoing)\n",
    "- Reply: Reply to a tweet\n",
    "- Quote: Sharing with some changes to the original post\n",
    "- Like: Liking the tweet\n",
    "and others but not relevant right now.  \n",
    "<br>\n",
    "\n",
    "In GNIP, you can track tweet, retweet, reply and a quote. Using *verb* field we can check if post is a tweet or a retweet.  \n",
    "- On GNIP, verb for tweet, reply and quote is **post** and verb for pure retweet is **share**.  \n",
    "*A pure retweet is a tweet which is not not tampered with i.e. tweet is just shared as it is. * \n",
    "<br><br>\n",
    "- Reply is identified by filtering verb using post and then checking if there is *inreplyto* field in it\n",
    "- Quoted Tweet is identified by filtering verb using post and then checking if there is *twitter_quoted_status* field in it, which contains the original tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retweet Dictionary \n",
    "retweetsDict = {'actorId':[],'displayName':[],'screenName':[],'statusesCount':[], 'favoritesCount':[],\n",
    "                'friendsCount':[], 'followersCount':[],'listedCounts':[], 'actorLanguages':[],'summary':[],\n",
    "                'createdAt':[],'verified':[],'location':[],'locationType':[],'verb':[],'tweetId':[],\n",
    "                'tweetFavCount':[], 'generator':[],'postedTime':[],'retweetCount':[],\n",
    "                'object.body':[],'object.tweetId':[],'object.actorId':[],'object.postedTime':[]\n",
    "               }\n",
    "\n",
    "# Tweet Dictionary\n",
    "tweetsDict = {'actorId':[],'displayName':[],'screenName':[],'statusesCount':[], 'favoritesCount':[],\n",
    "                'friendsCount':[], 'followersCount':[],'listedCounts':[], 'actorLanguages':[],'summary':[],\n",
    "                'createdAt':[],'verified':[],'location':[],'locationType':[],'body':[], 'verb':[],'tweetId':[],\n",
    "                'tweetFavCount':[], 'generator':[],'hashtags':[],'mentionIds':[],'mentionScreenNames':[],\n",
    "                'inReplyTo':[],'quotedTweetId':[],'quotedTweetUserId':[],'tweetLanguage':[],'postedTime':[],'retweetCount':[]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Somethings to keep in mind:\n",
    "Data extracted from Gnip is dependent on way you have done it. But I generally prefer extracting in multiple files, maybe 500 posts in each file and then storing in one rar.    \n",
    "  \n",
    "So, each JSON file has maximum of 500 posts. This makes sure that if one file gets corrupt, then you don't end up loosing lot of data.    \n",
    "  \n",
    "Now, there are two different ways people prefer storing their JSON files. Compressing each JSON file and then compressing that folder or just compressing that entire folder.  \n",
    "  \n",
    "I will show how to deal with both types of folders but there's not much difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When data is compressed in .gz format\n",
    "def extractDataGz(dataDict,function,path):\n",
    "    wasteFiles = [] # collects file names that have been either courrpt or files that encounter any errors while parsing\n",
    "    fileNo = 0\n",
    "    tweetNo = 0\n",
    "    allFiles = os.listdir(path)\n",
    "    for file in allFiles:\n",
    "        fileNo+=1\n",
    "        if fileNo %100 == 0: # printing progress\n",
    "            print(\"file: \",fileNo) # prints number of files parsed\n",
    "            print(\"data: \",len(dataDict['actorId'])) # prints data collected in dictionary\n",
    "\n",
    "        try:\n",
    "            with gzip.open(path+file, 'rb') as f1: #gzip is the only difference\n",
    "                file_content = f1.read()\n",
    "                file_content = json.loads(file_content)\n",
    "                function(dataDict,file_content['results'])\n",
    "\n",
    "            continue\n",
    "        except OSError: # if file is courrpt\n",
    "            wasteFiles.append(file)\n",
    "            continue\n",
    "            # Another error that can occur is when file is not JSON format and is OS file. Error encountered is JSONDecodeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When data is JSON format\n",
    "def extractData(dataDict,function,path):\n",
    "    wasteFiles = [] # collects file names that have been either courrpt or files that encounter any errors while parsing\n",
    "    fileNo = 0\n",
    "    tweetNo = 0\n",
    "    allFiles = os.listdir(path)\n",
    "    for file in allFiles:\n",
    "        fileNo+=1\n",
    "        if fileNo %100 == 0: # printing progress\n",
    "            print(\"file: \",fileNo) # prints number of files parsed\n",
    "            print(\"data: \",len(dataDict['actorId'])) # prints data collected in dictionary\n",
    "\n",
    "        try:\n",
    "            with open(path+file, 'rb') as f1:\n",
    "                file_content = f1.read()\n",
    "                file_content = json.loads(file_content)\n",
    "                function(dataDict,file_content['results'])\n",
    "\n",
    "            continue\n",
    "        except OSError: # if file is courrpt\n",
    "            wasteFiles.append(file)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweetData = pd.DataFrame(tweetsDict) #Use the dictionary that you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making sure you don't have duplicate data\n",
    "Sometimes, if you have made multiple pulls and try to combine those, it may happen that you might end up getting duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = list(tweetData.columns)# remove tweetId\n",
    "cols.remove('tweetId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempData = tweetData.groupby('tweetId',as_index=False)[cols]\n",
    "realTweetData = tempData.agg(lambda x: x.iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Data in python:\n",
    "\n",
    "##### Issue:\n",
    "While using pandas, both to_csv and read_csv functions are not very consistent. Now, issues occur while dealing with text data. Specifically, issues with line terminators.  \n",
    "Line terminators depend on type of Operating System used and can be as \"\\r\", \"\\n\", \"\\r\\n\".  \n",
    "\n",
    "For Macs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "realTweetData.to_csv(\"PATH WHERE YOU WANT TO SAVE YOUR FILE/filename.csv\",quoting=csv.QUOTE_NONNUMERIC, date_format='%Y-%m-%d %H:%M:%S', encoding='utf-8',line_terminator = '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
